{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caution: Scraping takes a long time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "from sklearn import preprocessing\n",
    "from sklearn import linear_model\n",
    "from sklearn import svm\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from scrape_weather import scrape_weather"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('health_inspect_cleaned.csv', index_col=0)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define custom function to scrape Weather Underground for temperature/humidity data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def scrape_weather(zipcode, inspection_date):\n",
    "    # Define the base URL for the Weather Underground historical archives\n",
    "    baseurl = 'https://www.wunderground.com/history/zipcode/{}/{}/{}/{}/DailyHistory.html' # zipcode, year, month, day\n",
    "\n",
    "    # Define regular expression we'll use later\n",
    "    tag_regex = re.compile(r'<.+>([0-9.,]+)<\\/.+>')\n",
    "    \n",
    "    # Initialize lists to store 3-day temperature and humidity\n",
    "    temperature = [np.nan]*3\n",
    "    humidity = [np.nan]*3\n",
    "\n",
    "    # Scrape temperature and humidity for inspection date and up to 2 days prior\n",
    "    for i in range(0,3):\n",
    "        # Subtract appropriate number of days\n",
    "        date = pd.to_datetime(inspection_date) - np.timedelta64(i,'D')\n",
    "\n",
    "        # Extract year, month, and day from datetime object\n",
    "        year = pd.to_datetime(str(date)).year\n",
    "        month = pd.to_datetime(str(date)).month\n",
    "        day = pd.to_datetime(str(date)).day\n",
    "\n",
    "        # Open URL and turn into BeautifulSoup\n",
    "        r = requests.get(baseurl.format(zipcode, year, month, day)).text\n",
    "        soup = BeautifulSoup(r, 'lxml')\n",
    "\n",
    "        # Find tags corresponding to average temperature and humidity\n",
    "        temperature_tag = soup.find('span', string='Mean Temperature')\n",
    "        if temperature_tag:\n",
    "            temperature_tag = temperature_tag.find_next(class_='wx-value')\n",
    "        \n",
    "        humidity_tag = soup.find('span', string='Average Humidity')\n",
    "        if humidity_tag:\n",
    "            humidity_tag = humidity_tag.find_next('td')\n",
    "\n",
    "        # Use regex to extract numerical value from tags\n",
    "        # Also convert to float\n",
    "        match = re.search(tag_regex, str(temperature_tag))\n",
    "        if match:\n",
    "            temperature[i] = float(match.group(1))\n",
    "        match = re.search(tag_regex, str(humidity_tag))\n",
    "        if match:\n",
    "            humidity[i] = float(match.group(1))\n",
    "\n",
    "    # Next two lines will cause RuntimeWarning if temperature and humidity are all NaN\n",
    "    avg_temperature = np.nanmean(temperature)\n",
    "    avg_humidity = np.nanmean(humidity)\n",
    "    \n",
    "    return avg_temperature, avg_humidity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterate over rows in dataframe to add 3-day average temperature and humidity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Change count to pick up where we left off if scraping didn't finish\n",
    "start = 2600\n",
    "rows_to_scrape = range(start,len(df))\n",
    "to_scrape = df.loc[df.index[rows_to_scrape], :]\n",
    "\n",
    "nrows = len(to_scrape)\n",
    "temp_3day = [np.nan]*nrows\n",
    "humidity_3day = [np.nan]*nrows\n",
    "inds = [np.nan]*nrows\n",
    "\n",
    "count = 0\n",
    "for index, row in to_scrape.iterrows():\n",
    "    # Keep track of progress\n",
    "    count += 1\n",
    "    print('Scraping for row {} of {} ({}% complete)'.format(count, nrows, int(100*(count/nrows)))\n",
    "    \n",
    "    # Scrape in a try-except block and move onto next row if can't scrape successfully\n",
    "    try:\n",
    "        temp_3day[count-1], humidity_3day[count-1] = scrape_weather(row['zipcode'], row['latest_inspection'])\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "    inds[count-1] = int(index)\n",
    "\n",
    "    if count % 100 == 0: # Save intermediate results every 100 rows\n",
    "        weather_df = pd.DataFrame({'3-day temp': temp_3day, '3-day humidity': humidity_3day}, index=inds)\n",
    "        weather_df.to_csv('../../weather_data.csv')\n",
    "\n",
    "weather_df = pd.DataFrame({'3-day temp': temp_3day, '3-day humidity': humidity_3day})\n",
    "weather_df.head()\n",
    "weather_df.to_csv('weather_data.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
